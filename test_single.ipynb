{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(code_str):\n",
    "    code_template = \"\"\"ä½ ç°åœ¨æ˜¯ä¸€ä¸ªç¨‹åºå‘˜åŠ©æ‰‹. ç¨‹åºå‘˜åœ¨å†™äº†ä»£ç ä¹‹åå¿˜è®°ç»™ä»£ç æ³¨é‡Š. ä½ çš„ä»»åŠ¡æ˜¯ç»™ç¨‹åºå‘˜çš„ä»£ç è¿›è¡Œåˆç†æ°å½“çš„ä¸­æ–‡æ³¨é‡Šã€‚è®©ç”¨æˆ·èƒ½å¤Ÿå¿«é€Ÿçš„ç†è§£ç›¸å…³çš„ä»£ç ã€‚åªè¾“å‡ºæ³¨é‡Šåçš„ä»£ç ã€‚ä¸è¾“å‡ºå…¶å®ƒä»»ä½•å¤šä½™çš„å†…å®¹.\n",
    "    Q:\n",
    "    class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "    A:\n",
    "    ```python\n",
    "    class StreamHandler(BaseCallbackHandler):\n",
    "    \\\"\\\"\\\"\n",
    "    StreamHandler ç±»ç”¨äºå¤„ç†ä»è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¶çš„æ–°æ ‡è®°å¹¶æ›´æ–°ä¸€ä¸ªå®¹å™¨çš„æ˜¾ç¤ºå†…å®¹ã€‚\n",
    "\n",
    "    è¿™ä¸ªç±»ç»§æ‰¿è‡ª BaseCallbackHandlerï¼Œæ˜¯ä¸€ä¸ªå›è°ƒå¤„ç†å™¨ï¼Œä¸“é—¨ç”¨äºå¤„ç†å’Œå“åº”æ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–°ç”Ÿæˆçš„æ ‡è®°ã€‚\n",
    "\n",
    "    Attributes:\n",
    "        container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºåœ¨å…¶ä¸­æ˜¾ç¤ºæ–‡æœ¬ã€‚\n",
    "        text: åˆå§‹æ–‡æœ¬ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚\n",
    "\n",
    "    Methods:\n",
    "        on_llm_new_token(token: str, **kwargs): å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è°ƒç”¨ã€‚å°†æ–°æ ‡è®°é™„åŠ åˆ°ç°æœ‰æ–‡æœ¬å¹¶æ›´æ–°å®¹å™¨çš„æ˜¾ç¤ºã€‚\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        \\\"\\\"\\\"\n",
    "        åˆå§‹åŒ– StreamHandler å®ä¾‹ã€‚\n",
    "\n",
    "        Args:\n",
    "            container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºæ˜¾ç¤ºæ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ã€‚\n",
    "            initial_text: å¯é€‰ï¼›åˆå§‹æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚\n",
    "        \\\"\\\"\\\"\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è¢«è°ƒç”¨çš„æ–¹æ³•ã€‚\n",
    "\n",
    "        è¿™ä¸ªæ–¹æ³•å°†æ¥æ”¶åˆ°çš„æ ‡è®°é™„åŠ åˆ°ç±»å®ä¾‹çš„æ–‡æœ¬å±æ€§ä¸Šï¼Œå¹¶æ›´æ–°å®¹å™¨ä¸­çš„æ˜¾ç¤ºå†…å®¹ã€‚\n",
    "\n",
    "        Args:\n",
    "            token: ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°çš„æ–°æ ‡è®°å­—ç¬¦ä¸²ã€‚\n",
    "            **kwargs: å¯é€‰å‚æ•°ï¼Œç”¨äºé¢å¤–çš„åŠŸèƒ½æˆ–å¤„ç†ã€‚\n",
    "        \\\"\\\"\\\"\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "        ```\n",
    "    Q:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", code_template),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=\"https://aiapi.xing-yun.cn/v1\",\n",
    "        openai_api_key=\"sk-RSAL5bknVmekLf005e714770B4Af431d821397F97d865cEb\",\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "    # llm = Ollama(model=\"yi:34b-chat\")\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"input\": code_str})\n",
    "    # print(f\"Response is {response.content}\")\n",
    "    # rt = extract_code(response.content)\n",
    "    # return rt\n",
    "    if not response.content:\n",
    "        return \"\"\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pprint import pprint \n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitter(file):\n",
    "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    java_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JAVA, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.CPP, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    Javascript_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JS, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    match file.split('.'):\n",
    "        case [_, 'py']:\n",
    "            print(\"Python splitter\")\n",
    "            return python_splitter\n",
    "        case [_, 'java']:\n",
    "            print(\"Java splitter\")\n",
    "            return java_splitter\n",
    "        case [_, 'cpp']:\n",
    "            print(\"C++ splitter\")\n",
    "            return cpp_splitter\n",
    "        case [_, 'js']:\n",
    "            print(\"Javascript splitter\")\n",
    "            return Javascript_splitter\n",
    "        case _:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python splitter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.text_splitter.RecursiveCharacterTextSplitter at 0x7f6d97359390>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = get_splitter(file_path)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\"data\", \"RiseGPT\", \"app.py\")\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "texts = splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  t in texts:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(t.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lst = [get_comment(t.page_content) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import streamlit as st\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "from utils import *\n",
      "import json\n",
      "import os \n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import (\n",
      "    ChatPromptTemplate,\n",
      "    MessagesPlaceholder,\n",
      ")\n",
      "from operator import itemgetter\n",
      "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
      "\n",
      "_template = \"\"\"Answer the user questions based on the context. \n",
      "<context>\n",
      "{context}\n",
      "<context/>\n",
      "\"\"\"\n",
      "# Prompt\n",
      "\n",
      "\n",
      "os.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\n",
      "st.title(\"RiseGPT\")\n",
      "with st.expander(\"â„¹ï¸ è¯´æ˜\"):\n",
      "    st.caption(\n",
      "        \"é‡åº†å¸‚è¥¿å—å¤§å­¦Riseå®éªŒå®¤åˆ˜å¿—æ˜è€å¸ˆè®ºæ–‡åŠ©æ‰‹(ç¬¬ä¸€æ¬¡åŠ è½½è¯·ç¨ç­‰)\"\n",
      "    )\n",
      "\n",
      "if \"openai_model\" not in st.session_state:\n",
      "    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\n",
      "\n",
      "if \"messages\" not in st.session_state:\n",
      "    st.session_state.messages = []\n",
      "\n",
      "if \"db\" not in st.session_state:\n",
      "    st.session_state.db = ingest()\n",
      "\n",
      "for message in st.session_state.messages:\n",
      "    with st.chat_message(message[\"role\"]):\n",
      "        st.markdown(message[\"content\"])\n",
      "\n",
      "# Maximum allowed messages\n",
      "max_messages = (\n",
      "    100  # Counting both user and assistant messages, so 10 iterations of conversation\n",
      ")\n",
      "```-----\n",
      "```python\n",
      "class StreamHandler(BaseCallbackHandler):\n",
      "    \"\"\"\n",
      "    StreamHandler ç±»ç”¨äºå¤„ç†ä»è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¶çš„æ–°æ ‡è®°å¹¶æ›´æ–°ä¸€ä¸ªå®¹å™¨çš„æ˜¾ç¤ºå†…å®¹ã€‚\n",
      "\n",
      "    è¿™ä¸ªç±»ç»§æ‰¿è‡ª BaseCallbackHandlerï¼Œæ˜¯ä¸€ä¸ªå›è°ƒå¤„ç†å™¨ï¼Œä¸“é—¨ç”¨äºå¤„ç†å’Œå“åº”æ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–°ç”Ÿæˆçš„æ ‡è®°ã€‚\n",
      "\n",
      "    Attributes:\n",
      "        container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºåœ¨å…¶ä¸­æ˜¾ç¤ºæ–‡æœ¬ã€‚\n",
      "        text: åˆå§‹æ–‡æœ¬ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚\n",
      "\n",
      "    Methods:\n",
      "        on_llm_new_token(token: str, **kwargs): å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è°ƒç”¨ã€‚å°†æ–°æ ‡è®°é™„åŠ åˆ°ç°æœ‰æ–‡æœ¬å¹¶æ›´æ–°å®¹å™¨çš„æ˜¾ç¤ºã€‚\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, container, initial_text=\"\"):\n",
      "        \"\"\"\n",
      "        åˆå§‹åŒ– StreamHandler å®ä¾‹ã€‚\n",
      "\n",
      "        Args:\n",
      "            container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºæ˜¾ç¤ºæ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ã€‚\n",
      "            initial_text: å¯é€‰ï¼›åˆå§‹æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚\n",
      "        \"\"\"\n",
      "        self.container = container\n",
      "        self.text = initial_text\n",
      "\n",
      "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
      "        \"\"\"\n",
      "        å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è¢«è°ƒç”¨çš„æ–¹æ³•ã€‚\n",
      "\n",
      "        è¿™ä¸ªæ–¹æ³•å°†æ¥æ”¶åˆ°çš„æ ‡è®°é™„åŠ åˆ°ç±»å®ä¾‹çš„æ–‡æœ¬å±æ€§ä¸Šï¼Œå¹¶æ›´æ–°å®¹å™¨ä¸­çš„æ˜¾ç¤ºå†…å®¹ã€‚\n",
      "\n",
      "        Args:\n",
      "            token: ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°çš„æ–°æ ‡è®°å­—ç¬¦ä¸²ã€‚\n",
      "            **kwargs: å¯é€‰å‚æ•°ï¼Œç”¨äºé¢å¤–çš„åŠŸèƒ½æˆ–å¤„ç†ã€‚\n",
      "        \"\"\"\n",
      "        self.text += token\n",
      "        self.container.markdown(self.text)\n",
      "\n",
      "\n",
      "def remain_last(input_str):\n",
      "    \"\"\"\n",
      "    ä»è¾“å…¥å­—ç¬¦ä¸²ä¸­æå–æœ€åä¸€éƒ¨åˆ†ã€‚\n",
      "\n",
      "    è¿™ä¸ªå‡½æ•°å°†è¾“å…¥å­—ç¬¦ä¸²æŒ‰ç…§æ–œæ ï¼ˆ/ï¼‰è¿›è¡Œåˆ†å‰²ï¼Œç„¶åè¿”å›æœ€åä¸€éƒ¨åˆ†ã€‚\n",
      "\n",
      "    Args:\n",
      "        input_str: è¾“å…¥å­—ç¬¦ä¸²ã€‚\n",
      "\n",
      "    Returns:\n",
      "        æœ€åä¸€éƒ¨åˆ†å­—ç¬¦ä¸²ã€‚\n",
      "    \"\"\"\n",
      "    parts = input_str.split(\"/\")\n",
      "    desired_part = parts[-1]\n",
      "    base_name, _ = os.path.splitext(desired_part)\n",
      "    return base_name\n",
      "```-----\n",
      "```python\n",
      "def get_response(question, docs):\n",
      "    \"\"\"\n",
      "    è·å–å¯¹ç»™å®šé—®é¢˜çš„å›ç­”ã€‚\n",
      "\n",
      "    Args:\n",
      "        question: è¦æé—®çš„é—®é¢˜ã€‚\n",
      "        docs: æ–‡æ¡£åˆ—è¡¨ï¼Œç”¨äºæä¾›ä¸Šä¸‹æ–‡ã€‚\n",
      "\n",
      "    Returns:\n",
      "        response: å¯¹é—®é¢˜çš„å›ç­”ã€‚\n",
      "    \"\"\"\n",
      "\n",
      "    # å°†æ–‡æ¡£å†…å®¹è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºä¸Šä¸‹æ–‡\n",
      "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
      "\n",
      "    # åˆ›å»º StreamHandler å®ä¾‹\n",
      "    stream_handler = StreamHandler(st.empty()) \n",
      "\n",
      "    # åˆ›å»º ChatOpenAI å®ä¾‹\n",
      "    chat_model = ChatOpenAI(\n",
      "            openai_api_base=\"https://aiapi.xing-yun.cn/v1\",\n",
      "            openai_api_key=\"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\",\n",
      "            streaming=True,  # ! important\n",
      "            callbacks=[stream_handler], # ! important\n",
      "            model_name=st.session_state.openai_model,\n",
      "        )\n",
      "\n",
      "    # åˆ›å»º ChatPromptTemplate å®ä¾‹\n",
      "    prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", _template),\n",
      "            MessagesPlaceholder(variable_name=\"history\"),\n",
      "            (\"human\", \"{input}\"),\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    # è·å–å†å²æ¶ˆæ¯\n",
      "    list_history = st.session_state.messages\n",
      "    list_history_str = json.dumps(list_history, ensure_ascii=False)\n",
      "\n",
      "    # åˆ›å»º ConversationBufferMemory å®ä¾‹\n",
      "    memory = ConversationBufferMemory(return_messages=True)\n",
      "\n",
      "    # ä¿å­˜ä¸Šä¸‹æ–‡\n",
      "    memory.save_context({\"input\":\"hi\"},{\"output\": list_history_str})\n",
      "\n",
      "    # åŠ è½½å†…å­˜å˜é‡\n",
      "    print(memory.load_memory_variables({}))\n",
      "\n",
      "    # åˆ›å»º RunnablePassthrough å®ä¾‹\n",
      "    chain = (\n",
      "        RunnablePassthrough.assign(\n",
      "            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
      "        )\n",
      "        | prompt\n",
      "        | chat_model\n",
      "    )\n",
      "\n",
      "    # è°ƒç”¨æ¨¡å‹è·å–å›ç­”\n",
      "    response = chain.invoke({\"input\": question, \"context\": context})\n",
      "    return response.content\n",
      "\n",
      "# å¦‚æœæ¶ˆæ¯æ•°é‡è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œåˆ™æç¤ºç”¨æˆ·ä¼‘æ¯\n",
      "if len(st.session_state.messages) >= max_messages:\n",
      "    st.info(\n",
      "        \"æ‚¨çš„ä½¿ç”¨æ¬¡æ•°è¿‡å¤šäº†ï¼Œè¯·ä¼‘æ¯ä¸€ä¸‹ï¼Œä¼‘æ¯å®Œåï¼Œè¯·é‡æ–°æ‰“å¼€ç½‘é¡µï¼Œç»§ç»­ä½¿ç”¨\"\n",
      "    )\n",
      "\n",
      "else:\n",
      "    # è·å–ç”¨æˆ·è¾“å…¥çš„é—®é¢˜\n",
      "    if question := st.chat_input(\"è¯·è¾“å…¥æ‚¨å¯¹åˆ˜å¿—æ˜è€å¸ˆæ–‡ç« çš„ç–‘é—®ï¼Œå¸Œæœ›èƒ½å¸®æ‚¨è§£æƒ‘\"):\n",
      "        # è·å–ç›¸å…³æ–‡æ¡£\n",
      "        docs = get_documents(question, st.session_state.db)\n",
      "\n",
      "        # å°†ç”¨æˆ·çš„é—®é¢˜æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­\n",
      "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
      "\n",
      "        # åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºç”¨æˆ·çš„é—®é¢˜\n",
      "        with st.chat_message(\"user\"):\n",
      "            st.markdown(question)\n",
      "```-----\n",
      "```python\n",
      "with st.chat_message(\"assistant\"):\n",
      "    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œç”¨äºå­˜å‚¨å®Œæ•´çš„å›å¤å†…å®¹\n",
      "    full_response = \"\"\n",
      "    # è°ƒç”¨ get_response å‡½æ•°è·å–å›å¤å†…å®¹ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ° full_response ä¸­\n",
      "    response = get_response(question, docs)\n",
      "    full_response += response\n",
      "    # å°†æ–‡æ¡£çš„æ¥æºä¿¡æ¯æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”æ¯ä¸ªæ¥æº\n",
      "    sources = \"\\n\\n\".join([f\"ğŸ“š æ¥æº {i + 1}: { remain_last( d.metadata['source'] ) } ç¬¬ {d.metadata['page']}é¡µ\" for i, d in enumerate(docs)])\n",
      "    # åˆ›å»ºä¸€ä¸ªç©ºçš„ message_placeholder å¯¹è±¡\n",
      "    message_placeholder = st.empty()\n",
      "    # åœ¨ message_placeholder ä¸­æ˜¾ç¤ºæ¥æºä¿¡æ¯\n",
      "    message_placeholder.markdown(sources)\n",
      "# å°†å®Œæ•´çš„å›å¤å†…å®¹æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€çš„ messages åˆ—è¡¨ä¸­\n",
      "st.session_state.messages.append(\n",
      "    {\"role\": \"assistant\", \"content\": full_response}\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "rt = \"-----\\n\".join(s_lst)\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "with st.chat_message(\"assistant\"):\n",
      "    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œç”¨äºå­˜å‚¨å®Œæ•´çš„å›å¤å†…å®¹\n",
      "    full_response = \"\"\n",
      "    # è°ƒç”¨ get_response å‡½æ•°è·å–å›å¤å†…å®¹ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ° full_response ä¸­\n",
      "    response = get_response(question, docs)\n",
      "    full_response += response\n",
      "    # å°†æ–‡æ¡£çš„æ¥æºä¿¡æ¯æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä½¿ç”¨æ¢è¡Œç¬¦åˆ†éš”æ¯ä¸ªæ¥æº\n",
      "    sources = \"\\n\\n\".join([f\"ğŸ“š æ¥æº {i + 1}: { remain_last( d.metadata['source'] ) } ç¬¬ {d.metadata['page']}é¡µ\" for i, d in enumerate(docs)])\n",
      "    # åˆ›å»ºä¸€ä¸ªç©ºçš„ message_placeholder å¯¹è±¡\n",
      "    message_placeholder = st.empty()\n",
      "    # åœ¨ message_placeholder ä¸­æ˜¾ç¤ºæ¥æºä¿¡æ¯\n",
      "    message_placeholder.markdown(sources)\n",
      "# å°†å®Œæ•´çš„å›å¤å†…å®¹æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€çš„ messages åˆ—è¡¨ä¸­\n",
      "st.session_state.messages.append(\n",
      "    {\"role\": \"assistant\", \"content\": full_response}\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(s_lst[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_lst[-1].count(\"cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nimport streamlit as st\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom utils import *\\nimport json\\nimport os \\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import (\\n    ChatPromptTemplate,\\n    MessagesPlaceholder,\\n)\\nfrom operator import itemgetter\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain.schema.runnable import RunnableLambda, RunnablePassthrough\\n\\n_template = \"\"\"Answer the user questions based on the context. \\n<context>\\n{context}\\n<context/>\\n\"\"\"\\n# Prompt\\n\\n\\nos.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \\nos.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\\nst.title(\"RiseGPT\")\\nwith st.expander(\"â„¹ï¸ è¯´æ˜\"):\\n    st.caption(\\n        \"é‡åº†å¸‚è¥¿å—å¤§å­¦Riseå®éªŒå®¤åˆ˜å¿—æ˜è€å¸ˆè®ºæ–‡åŠ©æ‰‹(ç¬¬ä¸€æ¬¡åŠ è½½è¯·ç¨ç­‰)\"\\n    )\\n\\nif \"openai_model\" not in st.session_state:\\n    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nif \"db\" not in st.session_state:\\n    st.session_state.db = ingest()\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# Maximum allowed messages\\nmax_messages = (\\n    100  # Counting both user and assistant messages, so 10 iterations of conversation\\n)\\n```'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_code(text):\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import streamlit as st\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "from utils import *\n",
      "import json\n",
      "import os \n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import (\n",
      "    ChatPromptTemplate,\n",
      "    MessagesPlaceholder,\n",
      ")\n",
      "from operator import itemgetter\n",
      "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
      "\n",
      "_template = \"\"\"Answer the user questions based on the context. \n",
      "<context>\n",
      "{context}\n",
      "<context/>\n",
      "\"\"\"\n",
      "# Prompt\n",
      "\n",
      "\n",
      "os.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\n",
      "st.title(\"RiseGPT\")\n",
      "with st.expander(\"â„¹ï¸ è¯´æ˜\"):\n",
      "    st.caption(\n",
      "        \"é‡åº†å¸‚è¥¿å—å¤§å­¦Riseå®éªŒå®¤åˆ˜å¿—æ˜è€å¸ˆè®ºæ–‡åŠ©æ‰‹(ç¬¬ä¸€æ¬¡åŠ è½½è¯·ç¨ç­‰)\"\n",
      "    )\n",
      "\n",
      "if \"openai_model\" not in st.session_state:\n",
      "    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\n",
      "\n",
      "if \"messages\" not in st.session_state:\n",
      "    st.session_state.messages = []\n",
      "\n",
      "if \"db\" not in st.session_state:\n",
      "    st.session_state.db = ingest()\n",
      "\n",
      "for message in st.session_state.messages:\n",
      "    with st.chat_message(message[\"role\"]):\n",
      "        st.markdown(message[\"content\"])\n",
      "\n",
      "# Maximum allowed messages\n",
      "max_messages = (\n",
      "    100  # Counting both user and assistant messages, so 10 iterations of conversation\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = s_lst[0]\n",
    "# print(f\"The str is {text}\")\n",
    "rt = extract_code(text)\n",
    "print(\"\\n\".join([r for r in rt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Language(Enum):\n",
    "    PYTHON = \"python\"\n",
    "    JAVA = \"java\"\n",
    "    CPP = \"cpp\"\n",
    "    JS = \"js\"\n",
    "\n",
    "python = Language.PYTHON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.PYTHON\n"
     ]
    }
   ],
   "source": [
    "print(python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "print(python.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpp\n"
     ]
    }
   ],
   "source": [
    "print(Language.CPP.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
