{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(code_str):\n",
    "    code_template = \"\"\"你现在是一个程序员助手. 程序员在写了代码之后忘记给代码注释. 你的任务是给程序员的代码进行合理恰当的中文注释。让用户能够快速的理解相关的代码。只输出注释后的代码。不输出其它任何多余的内容.\n",
    "    Q:\n",
    "    class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "    A:\n",
    "    ```python\n",
    "    class StreamHandler(BaseCallbackHandler):\n",
    "    \\\"\\\"\\\"\n",
    "    StreamHandler 类用于处理从语言模型（LLM）接收的新标记并更新一个容器的显示内容。\n",
    "\n",
    "    这个类继承自 BaseCallbackHandler，是一个回调处理器，专门用于处理和响应来自语言模型的新生成的标记。\n",
    "\n",
    "    Attributes:\n",
    "        container: 容器对象，用于在其中显示文本。\n",
    "        text: 初始文本，用于在容器中开始显示。默认为空字符串。\n",
    "\n",
    "    Methods:\n",
    "        on_llm_new_token(token: str, **kwargs): 当从语言模型接收到新标记时调用。将新标记附加到现有文本并更新容器的显示。\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        \\\"\\\"\\\"\n",
    "        初始化 StreamHandler 实例。\n",
    "\n",
    "        Args:\n",
    "            container: 容器对象，用于显示来自语言模型的文本。\n",
    "            initial_text: 可选；初始文本字符串，用于在容器中开始显示。默认为空字符串。\n",
    "        \\\"\\\"\\\"\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        当从语言模型接收到新标记时被调用的方法。\n",
    "\n",
    "        这个方法将接收到的标记附加到类实例的文本属性上，并更新容器中的显示内容。\n",
    "\n",
    "        Args:\n",
    "            token: 从语言模型接收到的新标记字符串。\n",
    "            **kwargs: 可选参数，用于额外的功能或处理。\n",
    "        \\\"\\\"\\\"\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "        ```\n",
    "    Q:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", code_template),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=\"https://aiapi.xing-yun.cn/v1\",\n",
    "        openai_api_key=\"sk-RSAL5bknVmekLf005e714770B4Af431d821397F97d865cEb\",\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "    # llm = Ollama(model=\"yi:34b-chat\")\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"input\": code_str})\n",
    "    # print(f\"Response is {response.content}\")\n",
    "    # rt = extract_code(response.content)\n",
    "    # return rt\n",
    "    if not response.content:\n",
    "        return \"\"\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pprint import pprint \n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitter(file):\n",
    "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    java_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JAVA, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.CPP, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    Javascript_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JS, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    match file.split('.'):\n",
    "        case [_, 'py']:\n",
    "            print(\"Python splitter\")\n",
    "            return python_splitter\n",
    "        case [_, 'java']:\n",
    "            print(\"Java splitter\")\n",
    "            return java_splitter\n",
    "        case [_, 'cpp']:\n",
    "            print(\"C++ splitter\")\n",
    "            return cpp_splitter\n",
    "        case [_, 'js']:\n",
    "            print(\"Javascript splitter\")\n",
    "            return Javascript_splitter\n",
    "        case _:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python splitter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.text_splitter.RecursiveCharacterTextSplitter at 0x7f6d97359390>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = get_splitter(file_path)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\"data\", \"RiseGPT\", \"app.py\")\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "texts = splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  t in texts:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(t.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lst = [get_comment(t.page_content) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import streamlit as st\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "from utils import *\n",
      "import json\n",
      "import os \n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import (\n",
      "    ChatPromptTemplate,\n",
      "    MessagesPlaceholder,\n",
      ")\n",
      "from operator import itemgetter\n",
      "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
      "\n",
      "_template = \"\"\"Answer the user questions based on the context. \n",
      "<context>\n",
      "{context}\n",
      "<context/>\n",
      "\"\"\"\n",
      "# Prompt\n",
      "\n",
      "\n",
      "os.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\n",
      "st.title(\"RiseGPT\")\n",
      "with st.expander(\"ℹ️ 说明\"):\n",
      "    st.caption(\n",
      "        \"重庆市西南大学Rise实验室刘志明老师论文助手(第一次加载请稍等)\"\n",
      "    )\n",
      "\n",
      "if \"openai_model\" not in st.session_state:\n",
      "    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\n",
      "\n",
      "if \"messages\" not in st.session_state:\n",
      "    st.session_state.messages = []\n",
      "\n",
      "if \"db\" not in st.session_state:\n",
      "    st.session_state.db = ingest()\n",
      "\n",
      "for message in st.session_state.messages:\n",
      "    with st.chat_message(message[\"role\"]):\n",
      "        st.markdown(message[\"content\"])\n",
      "\n",
      "# Maximum allowed messages\n",
      "max_messages = (\n",
      "    100  # Counting both user and assistant messages, so 10 iterations of conversation\n",
      ")\n",
      "```-----\n",
      "```python\n",
      "class StreamHandler(BaseCallbackHandler):\n",
      "    \"\"\"\n",
      "    StreamHandler 类用于处理从语言模型（LLM）接收的新标记并更新一个容器的显示内容。\n",
      "\n",
      "    这个类继承自 BaseCallbackHandler，是一个回调处理器，专门用于处理和响应来自语言模型的新生成的标记。\n",
      "\n",
      "    Attributes:\n",
      "        container: 容器对象，用于在其中显示文本。\n",
      "        text: 初始文本，用于在容器中开始显示。默认为空字符串。\n",
      "\n",
      "    Methods:\n",
      "        on_llm_new_token(token: str, **kwargs): 当从语言模型接收到新标记时调用。将新标记附加到现有文本并更新容器的显示。\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, container, initial_text=\"\"):\n",
      "        \"\"\"\n",
      "        初始化 StreamHandler 实例。\n",
      "\n",
      "        Args:\n",
      "            container: 容器对象，用于显示来自语言模型的文本。\n",
      "            initial_text: 可选；初始文本字符串，用于在容器中开始显示。默认为空字符串。\n",
      "        \"\"\"\n",
      "        self.container = container\n",
      "        self.text = initial_text\n",
      "\n",
      "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
      "        \"\"\"\n",
      "        当从语言模型接收到新标记时被调用的方法。\n",
      "\n",
      "        这个方法将接收到的标记附加到类实例的文本属性上，并更新容器中的显示内容。\n",
      "\n",
      "        Args:\n",
      "            token: 从语言模型接收到的新标记字符串。\n",
      "            **kwargs: 可选参数，用于额外的功能或处理。\n",
      "        \"\"\"\n",
      "        self.text += token\n",
      "        self.container.markdown(self.text)\n",
      "\n",
      "\n",
      "def remain_last(input_str):\n",
      "    \"\"\"\n",
      "    从输入字符串中提取最后一部分。\n",
      "\n",
      "    这个函数将输入字符串按照斜杠（/）进行分割，然后返回最后一部分。\n",
      "\n",
      "    Args:\n",
      "        input_str: 输入字符串。\n",
      "\n",
      "    Returns:\n",
      "        最后一部分字符串。\n",
      "    \"\"\"\n",
      "    parts = input_str.split(\"/\")\n",
      "    desired_part = parts[-1]\n",
      "    base_name, _ = os.path.splitext(desired_part)\n",
      "    return base_name\n",
      "```-----\n",
      "```python\n",
      "def get_response(question, docs):\n",
      "    \"\"\"\n",
      "    获取对给定问题的回答。\n",
      "\n",
      "    Args:\n",
      "        question: 要提问的问题。\n",
      "        docs: 文档列表，用于提供上下文。\n",
      "\n",
      "    Returns:\n",
      "        response: 对问题的回答。\n",
      "    \"\"\"\n",
      "\n",
      "    # 将文档内容连接成一个字符串作为上下文\n",
      "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
      "\n",
      "    # 创建 StreamHandler 实例\n",
      "    stream_handler = StreamHandler(st.empty()) \n",
      "\n",
      "    # 创建 ChatOpenAI 实例\n",
      "    chat_model = ChatOpenAI(\n",
      "            openai_api_base=\"https://aiapi.xing-yun.cn/v1\",\n",
      "            openai_api_key=\"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\",\n",
      "            streaming=True,  # ! important\n",
      "            callbacks=[stream_handler], # ! important\n",
      "            model_name=st.session_state.openai_model,\n",
      "        )\n",
      "\n",
      "    # 创建 ChatPromptTemplate 实例\n",
      "    prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", _template),\n",
      "            MessagesPlaceholder(variable_name=\"history\"),\n",
      "            (\"human\", \"{input}\"),\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    # 获取历史消息\n",
      "    list_history = st.session_state.messages\n",
      "    list_history_str = json.dumps(list_history, ensure_ascii=False)\n",
      "\n",
      "    # 创建 ConversationBufferMemory 实例\n",
      "    memory = ConversationBufferMemory(return_messages=True)\n",
      "\n",
      "    # 保存上下文\n",
      "    memory.save_context({\"input\":\"hi\"},{\"output\": list_history_str})\n",
      "\n",
      "    # 加载内存变量\n",
      "    print(memory.load_memory_variables({}))\n",
      "\n",
      "    # 创建 RunnablePassthrough 实例\n",
      "    chain = (\n",
      "        RunnablePassthrough.assign(\n",
      "            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
      "        )\n",
      "        | prompt\n",
      "        | chat_model\n",
      "    )\n",
      "\n",
      "    # 调用模型获取回答\n",
      "    response = chain.invoke({\"input\": question, \"context\": context})\n",
      "    return response.content\n",
      "\n",
      "# 如果消息数量超过最大限制，则提示用户休息\n",
      "if len(st.session_state.messages) >= max_messages:\n",
      "    st.info(\n",
      "        \"您的使用次数过多了，请休息一下，休息完后，请重新打开网页，继续使用\"\n",
      "    )\n",
      "\n",
      "else:\n",
      "    # 获取用户输入的问题\n",
      "    if question := st.chat_input(\"请输入您对刘志明老师文章的疑问，希望能帮您解惑\"):\n",
      "        # 获取相关文档\n",
      "        docs = get_documents(question, st.session_state.db)\n",
      "\n",
      "        # 将用户的问题添加到消息列表中\n",
      "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
      "\n",
      "        # 在聊天界面显示用户的问题\n",
      "        with st.chat_message(\"user\"):\n",
      "            st.markdown(question)\n",
      "```-----\n",
      "```python\n",
      "with st.chat_message(\"assistant\"):\n",
      "    # 初始化一个空字符串，用于存储完整的回复内容\n",
      "    full_response = \"\"\n",
      "    # 调用 get_response 函数获取回复内容，并将其添加到 full_response 中\n",
      "    response = get_response(question, docs)\n",
      "    full_response += response\n",
      "    # 将文档的来源信息拼接成一个字符串，并使用换行符分隔每个来源\n",
      "    sources = \"\\n\\n\".join([f\"📚 来源 {i + 1}: { remain_last( d.metadata['source'] ) } 第 {d.metadata['page']}页\" for i, d in enumerate(docs)])\n",
      "    # 创建一个空的 message_placeholder 对象\n",
      "    message_placeholder = st.empty()\n",
      "    # 在 message_placeholder 中显示来源信息\n",
      "    message_placeholder.markdown(sources)\n",
      "# 将完整的回复内容添加到会话状态的 messages 列表中\n",
      "st.session_state.messages.append(\n",
      "    {\"role\": \"assistant\", \"content\": full_response}\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "rt = \"-----\\n\".join(s_lst)\n",
    "print(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "with st.chat_message(\"assistant\"):\n",
      "    # 初始化一个空字符串，用于存储完整的回复内容\n",
      "    full_response = \"\"\n",
      "    # 调用 get_response 函数获取回复内容，并将其添加到 full_response 中\n",
      "    response = get_response(question, docs)\n",
      "    full_response += response\n",
      "    # 将文档的来源信息拼接成一个字符串，并使用换行符分隔每个来源\n",
      "    sources = \"\\n\\n\".join([f\"📚 来源 {i + 1}: { remain_last( d.metadata['source'] ) } 第 {d.metadata['page']}页\" for i, d in enumerate(docs)])\n",
      "    # 创建一个空的 message_placeholder 对象\n",
      "    message_placeholder = st.empty()\n",
      "    # 在 message_placeholder 中显示来源信息\n",
      "    message_placeholder.markdown(sources)\n",
      "# 将完整的回复内容添加到会话状态的 messages 列表中\n",
      "st.session_state.messages.append(\n",
      "    {\"role\": \"assistant\", \"content\": full_response}\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(s_lst[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_lst[-1].count(\"cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nimport streamlit as st\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom utils import *\\nimport json\\nimport os \\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import (\\n    ChatPromptTemplate,\\n    MessagesPlaceholder,\\n)\\nfrom operator import itemgetter\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain.schema.runnable import RunnableLambda, RunnablePassthrough\\n\\n_template = \"\"\"Answer the user questions based on the context. \\n<context>\\n{context}\\n<context/>\\n\"\"\"\\n# Prompt\\n\\n\\nos.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \\nos.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\\nst.title(\"RiseGPT\")\\nwith st.expander(\"ℹ️ 说明\"):\\n    st.caption(\\n        \"重庆市西南大学Rise实验室刘志明老师论文助手(第一次加载请稍等)\"\\n    )\\n\\nif \"openai_model\" not in st.session_state:\\n    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nif \"db\" not in st.session_state:\\n    st.session_state.db = ingest()\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# Maximum allowed messages\\nmax_messages = (\\n    100  # Counting both user and assistant messages, so 10 iterations of conversation\\n)\\n```'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_code(text):\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import streamlit as st\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "from utils import *\n",
      "import json\n",
      "import os \n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import (\n",
      "    ChatPromptTemplate,\n",
      "    MessagesPlaceholder,\n",
      ")\n",
      "from operator import itemgetter\n",
      "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
      "\n",
      "_template = \"\"\"Answer the user questions based on the context. \n",
      "<context>\n",
      "{context}\n",
      "<context/>\n",
      "\"\"\"\n",
      "# Prompt\n",
      "\n",
      "\n",
      "os.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\n",
      "st.title(\"RiseGPT\")\n",
      "with st.expander(\"ℹ️ 说明\"):\n",
      "    st.caption(\n",
      "        \"重庆市西南大学Rise实验室刘志明老师论文助手(第一次加载请稍等)\"\n",
      "    )\n",
      "\n",
      "if \"openai_model\" not in st.session_state:\n",
      "    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\n",
      "\n",
      "if \"messages\" not in st.session_state:\n",
      "    st.session_state.messages = []\n",
      "\n",
      "if \"db\" not in st.session_state:\n",
      "    st.session_state.db = ingest()\n",
      "\n",
      "for message in st.session_state.messages:\n",
      "    with st.chat_message(message[\"role\"]):\n",
      "        st.markdown(message[\"content\"])\n",
      "\n",
      "# Maximum allowed messages\n",
      "max_messages = (\n",
      "    100  # Counting both user and assistant messages, so 10 iterations of conversation\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = s_lst[0]\n",
    "# print(f\"The str is {text}\")\n",
    "rt = extract_code(text)\n",
    "print(\"\\n\".join([r for r in rt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Language(Enum):\n",
    "    PYTHON = \"python\"\n",
    "    JAVA = \"java\"\n",
    "    CPP = \"cpp\"\n",
    "    JS = \"js\"\n",
    "\n",
    "python = Language.PYTHON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.PYTHON\n"
     ]
    }
   ],
   "source": [
    "print(python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "print(python.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpp\n"
     ]
    }
   ],
   "source": [
    "print(Language.CPP.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
