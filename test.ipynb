{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./data/RiseGPT/requirements.txt'\n",
      "'./data/RiseGPT/utils.py'\n",
      "the current file is utils.py\n",
      "def load_pdf(directory):\n",
      "    documents = []\n",
      "    for filename in os.listdir(directory):\n",
      "        if filename.endswith(\".pdf\"):\n",
      "            print(f\"Begin loading {filename}\")\n",
      "            filepath = os.path.join(directory, filename)\n",
      "            loader = PyPDFLoader(filepath)\n",
      "            document = loader.load()\n",
      "            documents.extend(document)\n",
      "            print(f\"{filename} load successfully\")\n",
      "    return documents\n",
      "def delete_space(path):\n",
      "    import os\n",
      "\n",
      "    folder_path = path\n",
      "\n",
      "    for root, dirs, files in os.walk(folder_path):\n",
      "        for file in files:\n",
      "            if file.endswith('.txt'):\n",
      "                file_path = os.path.join(root, file)\n",
      "\n",
      "                with open(file_path, 'r') as f:\n",
      "                    content = f.read()\n",
      "\n",
      "                processed_content = content.replace(' ', '').replace('\\n', '')\n",
      "\n",
      "                with open(file_path, 'w') as f:\n",
      "                    f.write(processed_content)\n",
      "\n",
      "    print(\"处理完成!\")\n",
      "def tiktoken_len(text):\n",
      "    tokenizer = tiktoken.get_encoding('p50k_base')\n",
      "    tokens = tokenizer.encode(\n",
      "        text,\n",
      "        disallowed_special=()\n",
      "    )\n",
      "    return len(tokens)\n",
      "def clean_string(input_doc):\n",
      "    placeholder = \"<DOUBLE_NEWLINE>\"\n",
      "    step1 = input_doc.page_content.replace(\"\\n\\n\", placeholder)\n",
      "    step2 = step1.replace(\"\\n\", \"\").replace(\" \", \"\").replace(\".\", \"\").replace('\\x00', \"\")\n",
      "    result = step2.replace(placeholder, \"\\n\\n\")\n",
      "    cleaned_text = re.sub('[\\ue000-\\uf8ff]', '', result)\n",
      "    final_text = re.sub(r'\\s+', \"\", cleaned_text)\n",
      "    threshold = 10\n",
      "    cleaned_content = re.sub(\n",
      "        r'[A-Za-z0-9$%!@#^&*]{' + str(threshold) + ',}', '', final_text)\n",
      "    input_doc.page_content = cleaned_content\n",
      "    input_doc.metadata = input_doc.metadata\n",
      "    return input_doc\n",
      "def ingest():\n",
      "\n",
      "    PATH = \"data/vector_src\"\n",
      "    print(\"Loading data...\")\n",
      "\n",
      "    if not os.path.exists(PATH):\n",
      "        os.makedirs(PATH)\n",
      "\n",
      "    persist_directory = PATH + \"/\" +  \"lzm-vectorstore/\"\n",
      "    folder_path = \"data/docs/\" + \"lzm/\"  # path to xx_src\n",
      "    loader = PyPDFDirectoryLoader(folder_path)\n",
      "    text_docs = DirectoryLoader(path=folder_path, glob=\"**/*.txt\").load()\n",
      "    raw_documents = loader.load()\n",
      "    raw_documents.extend(text_docs)\n",
      "\n",
      "    print(\"Splitting text...\")\n",
      "    text_splitter = RecursiveCharacterTextSplitter(\n",
      "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
      "        chunk_size=1000,\n",
      "        chunk_overlap=50,\n",
      "        length_function=tiktoken_len,\n",
      "    )\n",
      "    documents = text_splitter.split_documents(raw_documents)\n",
      "\n",
      "    documents = list(map(lambda doc: clean_string(doc), documents))\n",
      "    print(documents[0])\n",
      "    print(\"Creating vectorstore...\")\n",
      "    embeddings = OpenAIEmbeddings()\n",
      "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
      "    return vectorstore\n",
      "def get_documents(question, db):\n",
      "    retriver = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3})\n",
      "    docs = retriver.get_relevant_documents(query=question)\n",
      "    return docs\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.document_loaders import UnstructuredFileLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "import os\n",
      "import tiktoken  # !pip install tiktoken\n",
      "import sys\n",
      "import re\n",
      "import os\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Code for: def load_pdf(directory):\n",
      "\n",
      "\n",
      "# Code for: def delete_space(path):\n",
      "\n",
      "\n",
      "\n",
      "# Code for: def tiktoken_len(text):\n",
      "\n",
      "\n",
      "# Code for: def clean_string(input_doc):\n",
      "\n",
      "\n",
      "\n",
      "# Code for: def ingest():\n",
      "\n",
      "\n",
      "# Code for: def get_documents(question, db):\n",
      "'new_file_dir is ./data/new_risegpt'\n",
      "'./data/RiseGPT/app.py'\n",
      "the current file is app.py\n",
      "class StreamHandler(BaseCallbackHandler):\n",
      "    def __init__(self, container, initial_text=\"\"):\n",
      "        self.container = container\n",
      "        self.text = initial_text\n",
      "\n",
      "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
      "        self.text += token\n",
      "        self.container.markdown(self.text)\n",
      "def remain_last(input_str):\n",
      "    parts = input_str.split(\"/\")\n",
      "    desired_part = parts[-1]\n",
      "    base_name, _ = os.path.splitext(desired_part)\n",
      "    return base_name\n",
      "def get_response(question, docs):\n",
      "\n",
      "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
      "    stream_handler = StreamHandler(st.empty()) \n",
      "    chat_model = ChatOpenAI(\n",
      "            openai_api_base=\"https://aiapi.xing-yun.cn/v1\",\n",
      "            openai_api_key=\"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\",\n",
      "            streaming=True,  # ! important\n",
      "            callbacks=[stream_handler], # ! important\n",
      "            model_name=st.session_state.openai_model,\n",
      "        )\n",
      "    prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", _template),\n",
      "            MessagesPlaceholder(variable_name=\"history\"),\n",
      "            (\"human\", \"{input}\"),\n",
      "        ]\n",
      "    )\n",
      "\n",
      "\n",
      "    list_history = st.session_state.messages\n",
      "    list_history_str = json.dumps(list_history, ensure_ascii=False)\n",
      "    memory = ConversationBufferMemory(return_messages=True)\n",
      "    memory.save_context({\"input\":\"hi\"},{\"output\": list_history_str})\n",
      "    print(memory.load_memory_variables({}))\n",
      "    chain = (\n",
      "        RunnablePassthrough.assign(\n",
      "            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
      "        )\n",
      "        | prompt\n",
      "        | chat_model\n",
      "    )\n",
      "\n",
      "    response = chain.invoke({\"input\": question, \"context\": context})\n",
      "    return response.content\n",
      "import streamlit as st\n",
      "from langchain.callbacks.base import BaseCallbackHandler\n",
      "from utils import *\n",
      "import json\n",
      "import os \n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import (\n",
      "    ChatPromptTemplate,\n",
      "    MessagesPlaceholder,\n",
      ")\n",
      "from operator import itemgetter\n",
      "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
      "\n",
      "_template = \"\"\"Answer the user questions based on the context. \n",
      "<context>\n",
      "{context}\n",
      "<context/>\n",
      "\"\"\"\n",
      "# Prompt\n",
      "\n",
      "\n",
      "os.environ[\"OPENAI_API_BASE\"] = \"https://aiapi.xing-yun.cn/v1\" \n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\"\n",
      "st.title(\"RiseGPT\")\n",
      "with st.expander(\"ℹ️ 说明\"):\n",
      "    st.caption(\n",
      "        \"重庆市西南大学Rise实验室刘志明老师论文助手(第一次加载请稍等)\"\n",
      "    )\n",
      "\n",
      "if \"openai_model\" not in st.session_state:\n",
      "    st.session_state[\"openai_model\"] = \"gpt-4-1106-preview\"\n",
      "\n",
      "if \"messages\" not in st.session_state:\n",
      "    st.session_state.messages = []\n",
      "\n",
      "if \"db\" not in st.session_state:\n",
      "    st.session_state.db = ingest()\n",
      "\n",
      "for message in st.session_state.messages:\n",
      "    with st.chat_message(message[\"role\"]):\n",
      "        st.markdown(message[\"content\"])\n",
      "\n",
      "# Maximum allowed messages\n",
      "max_messages = (\n",
      "    100  # Counting both user and assistant messages, so 10 iterations of conversation\n",
      ")\n",
      "# Code for: class StreamHandler(BaseCallbackHandler):\n",
      "\n",
      "# Code for: def remain_last(input_str):\n",
      "\n",
      "# Code for: def get_response(question, docs):\n",
      "\n",
      "# persist_directory = \"data/vector_src/lzm-vectorstore\"\n",
      "if len(st.session_state.messages) >= max_messages:\n",
      "    st.info(\n",
      "        \"您的使用次数过多了，请休息一下，休息完后，请重新打开网页，继续使用\"\n",
      "    )\n",
      "\n",
      "else:\n",
      "    if question := st.chat_input(\"请输入您对刘志明老师文章的疑问，希望能帮您解惑\"):\n",
      "        docs = get_documents(question, st.session_state.db)\n",
      "st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
      "        with st.chat_message(\"user\"):\n",
      "            st.markdown(question)\n",
      "\n",
      "        with st.chat_message(\"assistant\"):\n",
      "            full_response = \"\"\n",
      "            response = get_response(question, docs)\n",
      "            full_response += response\n",
      "            sources = \"\\n\\n\".join([f\"📚 来源 {i + 1}: { remain_last( d.metadata['source'] ) } 第 {d.metadata['page']}页\" for i, d in enumerate(docs)])\n",
      "            message_placeholder = st.empty()\n",
      "            message_placeholder.markdown(sources)\n",
      "        st.session_state.messages.append(\n",
      "            {\"role\": \"assistant\", \"content\": full_response}\n",
      "        )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dell/Projects/code-comments/test.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(new_repo_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# 处理仓库\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m process_repository(source_repo_path, new_repo_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m pprint(\u001b[39m\"\u001b[39m\u001b[39m仓库处理完成。原仓库文件已复制并处理到新仓库。\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/dell/Projects/code-comments/test.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# 检查文件扩展名是否为代码文件\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m file_path\u001b[39m.\u001b[39mendswith((\u001b[39m'\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.java\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.cpp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.c\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# 读取并处理代码\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     processed_code \u001b[39m=\u001b[39m process(memory, root, file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# 构造新仓库中的相同路径\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     new_file_path \u001b[39m=\u001b[39m file_path\u001b[39m.\u001b[39mreplace(source_repo, new_repo)\n",
      "\u001b[1;32m/home/dell/Projects/code-comments/test.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe current file is \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([text\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m processed_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([get_comment(memory, text\u001b[39m.\u001b[39mpage_content) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m processed_code\n",
      "\u001b[1;32m/home/dell/Projects/code-comments/test.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe current file is \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([text\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m processed_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([get_comment(memory, text\u001b[39m.\u001b[39;49mpage_content) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m processed_code\n",
      "\u001b[1;32m/home/dell/Projects/code-comments/test.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     openai_api_base\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://aiapi.xing-yun.cn/v1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     openai_api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-1106\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m chain \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     RunnablePassthrough\u001b[39m.\u001b[39massign(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         history\u001b[39m=\u001b[39mRunnableLambda(memory\u001b[39m.\u001b[39mload_memory_variables) \u001b[39m|\u001b[39m itemgetter(\u001b[39m\"\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m|\u001b[39m llm\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: code_str})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: code_str}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/Projects/code-comments/test.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context(inputs, {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: response\u001b[39m.\u001b[39mcontent})\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/schema/runnable/base.py:1213\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1213\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1214\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1215\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1216\u001b[0m             patch_config(\n\u001b[1;32m   1217\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1218\u001b[0m             ),\n\u001b[1;32m   1219\u001b[0m         )\n\u001b[1;32m   1220\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/base.py:142\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    139\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    141\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    143\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    144\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    145\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    146\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    147\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    148\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    149\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    150\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    151\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/openai.py:412\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    411\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 412\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    413\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    414\u001b[0m )\n\u001b[1;32m    415\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/langchain/chat_models/openai.py:334\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    336\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    338\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/openai/resources/chat/completions.py:594\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    593\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    595\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    596\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    597\u001b[0m             {\n\u001b[1;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    616\u001b[0m             },\n\u001b[1;32m    617\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    618\u001b[0m         ),\n\u001b[1;32m    619\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    620\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    621\u001b[0m         ),\n\u001b[1;32m    622\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    623\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    625\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/openai/_base_client.py:858\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(request, auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    859\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    860\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[1;32m    861\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    902\u001b[0m     request,\n\u001b[1;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    906\u001b[0m )\n\u001b[1;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    930\u001b[0m         request,\n\u001b[1;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py:344\u001b[0m, in \u001b[0;36mTunnelHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection \u001b[39m=\u001b[39m HTTP11Connection(\n\u001b[1;32m    338\u001b[0m                 origin\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_origin,\n\u001b[1;32m    339\u001b[0m                 stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    340\u001b[0m                 keepalive_expiry\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keepalive_expiry,\n\u001b[1;32m    341\u001b[0m             )\n\u001b[1;32m    343\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1166\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pprint import pprint \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def process_repository(source_repo, new_repo):\n",
    "    memory = ConversationBufferMemory(return_messages=True)\n",
    "    # 遍历源仓库中的所有文件和目录\n",
    "    for root, dirs, files in os.walk(source_repo):\n",
    "        # 对每个文件进行处理\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            pprint(file_path)\n",
    "            # 检查文件扩展名是否为代码文件\n",
    "            if file_path.endswith(('.py', '.java', '.cpp', '.c')):\n",
    "                # 读取并处理代码\n",
    "                processed_code = process(memory, root, file)\n",
    "                # 构造新仓库中的相同路径\n",
    "                new_file_path = file_path.replace(source_repo, new_repo)\n",
    "                new_file_dir = os.path.dirname(new_file_path)\n",
    "                pprint(f\"new_file_dir is {new_file_dir}\")\n",
    "                if not os.path.exists(new_file_dir):\n",
    "                    os.makedirs(new_file_dir)\n",
    "                # 将处理后的代码写入新仓库\n",
    "                with open(new_file_path, 'w') as new_file:\n",
    "                    new_file.write(processed_code)\n",
    "            else:\n",
    "                # 对于非代码文件，直接复制\n",
    "                new_file_path = file_path.replace(source_repo, new_repo)\n",
    "                new_file_dir = os.path.dirname(new_file_path)\n",
    "                if not os.path.exists(new_file_dir):\n",
    "                    os.makedirs(new_file_dir)\n",
    "                shutil.copy(file_path, new_file_path)\n",
    "\n",
    "# 设置源仓库和新仓库的路径\n",
    "source_repo_path = './data/RiseGPT'\n",
    "new_repo_path = './data/new_risegpt'\n",
    "\n",
    "# 确保新仓库目录存在\n",
    "if not os.path.exists(new_repo_path):\n",
    "    os.makedirs(new_repo_path)\n",
    "\n",
    "# 处理仓库\n",
    "process_repository(source_repo_path, new_repo_path)\n",
    "\n",
    "pprint(\"仓库处理完成。原仓库文件已复制并处理到新仓库。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process(memory, root, file):\n",
    "    # 这里是处理代码的函数，具体实现依据需要填写\n",
    "    # 示例：简单地返回原始代码\n",
    "    loader = GenericLoader.from_filesystem(\n",
    "        root,\n",
    "        glob=file,\n",
    "        suffixes=[\".py\", \".cpp\", \".java\", \".c\", \".js\"],\n",
    "        parser=LanguageParser(),\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    splitter = get_splitter(file)\n",
    "    texts = splitter.split_documents(documents)\n",
    "    print(f\"the current file is {file}\")\n",
    "    print(\"\\n\".join([text.page_content for text in texts]))\n",
    "    processed_code = \"\\n\\n\".join([get_comment(memory, text.page_content) for text in texts])\n",
    "    return processed_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitter(file):\n",
    "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    java_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JAVA, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.CPP, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    Javascript_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JS, chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    match file.split('.'):\n",
    "        case [_, 'py']:\n",
    "            return python_splitter\n",
    "        case [_, 'java']:\n",
    "            return java_splitter\n",
    "        case [_, 'cpp']:\n",
    "            return cpp_splitter\n",
    "        case [_, 'js']:\n",
    "            return Javascript_splitter\n",
    "        case _:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(memory, code_str):\n",
    "    code_template = \"\"\"你现在是一个程序员，你的任务是给用户的代码进行详细中文注释。并且只输出注释后的代码。\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", code_template),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=\"https://aiapi.xing-yun.cn/v1\",\n",
    "        openai_api_key=\"sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a\",\n",
    "        model_name=\"gpt-3.5-turbo-1106\",\n",
    "        temperature=0\n",
    "    )\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n",
    "    response = chain.invoke({\"input\": code_str})\n",
    "    inputs = {\"input\": code_str}\n",
    "    memory.save_context(inputs, {\"output\": response.content})\n",
    "    rt = extract_code(response.content)\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(text):\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
