
import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from utils import *
import json
import os 
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import Chroma
from langchain.chains import LLMChain
from langchain.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
)
from operator import itemgetter
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough

_template = """Answer the user questions based on the context. 
<context>
{context}
<context/>
"""
# Prompt


os.environ["OPENAI_API_BASE"] = "https://aiapi.xing-yun.cn/v1" 
os.environ["OPENAI_API_KEY"] = "sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a"
st.title("RiseGPT")
with st.expander("â„¹ï¸ è¯´æ˜"):
    st.caption(
        "é‡åº†å¸‚è¥¿å—å¤§å­¦Riseå®éªŒå®¤åˆ˜å¿—æ˜è€å¸ˆè®ºæ–‡åŠ©æ‰‹(ç¬¬ä¸€æ¬¡åŠ è½½è¯·ç¨ç­‰)"
    )

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-4-1106-preview"

if "messages" not in st.session_state:
    st.session_state.messages = []

if "db" not in st.session_state:
    st.session_state.db = ingest()

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Maximum allowed messages
max_messages = (
    100  # Counting both user and assistant messages, so 10 iterations of conversation
)


class StreamHandler(BaseCallbackHandler):
    """
    StreamHandler ç±»ç”¨äºå¤„ç†ä»è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¶çš„æ–°æ ‡è®°å¹¶æ›´æ–°ä¸€ä¸ªå®¹å™¨çš„æ˜¾ç¤ºå†…å®¹ã€‚

    è¿™ä¸ªç±»ç»§æ‰¿è‡ª BaseCallbackHandlerï¼Œæ˜¯ä¸€ä¸ªå›è°ƒå¤„ç†å™¨ï¼Œä¸“é—¨ç”¨äºå¤„ç†å’Œå“åº”æ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–°ç”Ÿæˆçš„æ ‡è®°ã€‚

    Attributes:
        container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºåœ¨å…¶ä¸­æ˜¾ç¤ºæ–‡æœ¬ã€‚
        text: åˆå§‹æ–‡æœ¬ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚

    Methods:
        on_llm_new_token(token: str, **kwargs): å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è°ƒç”¨ã€‚å°†æ–°æ ‡è®°é™„åŠ åˆ°ç°æœ‰æ–‡æœ¬å¹¶æ›´æ–°å®¹å™¨çš„æ˜¾ç¤ºã€‚
    """

    def __init__(self, container, initial_text=""):
        """
        åˆå§‹åŒ– StreamHandler å®ä¾‹ã€‚

        Args:
            container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºæ˜¾ç¤ºæ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ã€‚
            initial_text: å¯é€‰ï¼›åˆå§‹æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
        """
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è¢«è°ƒç”¨çš„æ–¹æ³•ã€‚

        è¿™ä¸ªæ–¹æ³•å°†æ¥æ”¶åˆ°çš„æ ‡è®°é™„åŠ åˆ°ç±»å®ä¾‹çš„æ–‡æœ¬å±æ€§ä¸Šï¼Œå¹¶æ›´æ–°å®¹å™¨ä¸­çš„æ˜¾ç¤ºå†…å®¹ã€‚

        Args:
            token: ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°çš„æ–°æ ‡è®°å­—ç¬¦ä¸²ã€‚
            **kwargs: å¯é€‰å‚æ•°ï¼Œç”¨äºé¢å¤–çš„åŠŸèƒ½æˆ–å¤„ç†ã€‚
        """
        self.text += token
        self.container.markdown(self.text)



class StreamHandler(BaseCallbackHandler):
    """
    StreamHandler ç±»ç”¨äºå¤„ç†ä»è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¶çš„æ–°æ ‡è®°å¹¶æ›´æ–°ä¸€ä¸ªå®¹å™¨çš„æ˜¾ç¤ºå†…å®¹ã€‚

    è¿™ä¸ªç±»ç»§æ‰¿è‡ª BaseCallbackHandlerï¼Œæ˜¯ä¸€ä¸ªå›è°ƒå¤„ç†å™¨ï¼Œä¸“é—¨ç”¨äºå¤„ç†å’Œå“åº”æ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–°ç”Ÿæˆçš„æ ‡è®°ã€‚

    Attributes:
        container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºåœ¨å…¶ä¸­æ˜¾ç¤ºæ–‡æœ¬ã€‚
        text: åˆå§‹æ–‡æœ¬ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚

    Methods:
        on_llm_new_token(token: str, **kwargs): å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è°ƒç”¨ã€‚å°†æ–°æ ‡è®°é™„åŠ åˆ°ç°æœ‰æ–‡æœ¬å¹¶æ›´æ–°å®¹å™¨çš„æ˜¾ç¤ºã€‚
    """

    def __init__(self, container, initial_text=""):
        """
        åˆå§‹åŒ– StreamHandler å®ä¾‹ã€‚

        Args:
            container: å®¹å™¨å¯¹è±¡ï¼Œç”¨äºæ˜¾ç¤ºæ¥è‡ªè¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ã€‚
            initial_text: å¯é€‰ï¼›åˆå§‹æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œç”¨äºåœ¨å®¹å™¨ä¸­å¼€å§‹æ˜¾ç¤ºã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
        """
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        å½“ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°æ–°æ ‡è®°æ—¶è¢«è°ƒç”¨çš„æ–¹æ³•ã€‚

        è¿™ä¸ªæ–¹æ³•å°†æ¥æ”¶åˆ°çš„æ ‡è®°é™„åŠ åˆ°ç±»å®ä¾‹çš„æ–‡æœ¬å±æ€§ä¸Šï¼Œå¹¶æ›´æ–°å®¹å™¨ä¸­çš„æ˜¾ç¤ºå†…å®¹ã€‚

        Args:
            token: ä»è¯­è¨€æ¨¡å‹æ¥æ”¶åˆ°çš„æ–°æ ‡è®°å­—ç¬¦ä¸²ã€‚
            **kwargs: å¯é€‰å‚æ•°ï¼Œç”¨äºé¢å¤–çš„åŠŸèƒ½æˆ–å¤„ç†ã€‚
        """
        self.text += token
        self.container.markdown(self.text)


def remain_last(input_str):
    """
    ä»ç»™å®šçš„å­—ç¬¦ä¸²ä¸­æå–æœ€åä¸€ä¸ªéƒ¨åˆ†ã€‚

    è¿™ä¸ªå‡½æ•°å°†ç»™å®šçš„å­—ç¬¦ä¸²æŒ‰ç…§æ–œæ ï¼ˆ/ï¼‰è¿›è¡Œåˆ†å‰²ï¼Œç„¶åè¿”å›æœ€åä¸€ä¸ªéƒ¨åˆ†ã€‚

    Args:
        input_str: ç»™å®šçš„å­—ç¬¦ä¸²ã€‚

    Returns:
        æœ€åä¸€ä¸ªéƒ¨åˆ†çš„å­—ç¬¦ä¸²ã€‚
    """
    parts = input_str.split("/")
    desired_part = parts[-1]
    base_name, _ = os.path.splitext(desired_part)
    return base_name



def get_response(question, docs):
    """
    è·å–å¯¹ç»™å®šé—®é¢˜çš„å›ç­”ã€‚

    Args:
        question: é—®é¢˜å­—ç¬¦ä¸²ã€‚
        docs: æ–‡æ¡£åˆ—è¡¨ã€‚

    Returns:
        å›ç­”å­—ç¬¦ä¸²ã€‚
    """

    # å°†æ–‡æ¡£å†…å®¹è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºä¸Šä¸‹æ–‡
    context = "\n".join([doc.page_content for doc in docs])

    # åˆ›å»ºä¸€ä¸ª StreamHandler å®ä¾‹
    stream_handler = StreamHandler(st.empty()) 

    # åˆ›å»ºä¸€ä¸ª ChatOpenAI å®ä¾‹
    chat_model = ChatOpenAI(
            openai_api_base="https://aiapi.xing-yun.cn/v1",
            openai_api_key="sk-3e5wTBAl2iFDvQvW9b5693C90a97425eBf3b4bEa558eC66a",
            streaming=True,  # ! important
            callbacks=[stream_handler], # ! important
            model_name=st.session_state.openai_model,
        )

    # åˆ›å»º ChatPromptTemplate å®ä¾‹
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", _template),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
        ]
    )

    # è·å–å†å²æ¶ˆæ¯
    list_history = st.session_state.messages
    list_history_str = json.dumps(list_history, ensure_ascii=False)

    # åˆ›å»º ConversationBufferMemory å®ä¾‹å¹¶ä¿å­˜ä¸Šä¸‹æ–‡
    memory = ConversationBufferMemory(return_messages=True)
    memory.save_context({"input":"hi"},{"output": list_history_str})

    # åŠ è½½å†…å­˜å˜é‡
    print(memory.load_memory_variables({}))

    # åˆ›å»º RunnablePassthrough å®ä¾‹
    chain = (
        RunnablePassthrough.assign(
            history=RunnableLambda(memory.load_memory_variables) | itemgetter("history")
        )
        | prompt
        | chat_model
    )

    # è°ƒç”¨æ¨¡å‹è·å–å›ç­”
    response = chain.invoke({"input": question, "context": context})
    return response.content

# persist_directory = "data/vector_src/lzm-vectorstore"

# æ£€æŸ¥æ¶ˆæ¯æ•°é‡æ˜¯å¦è¶…è¿‡é™åˆ¶
if len(st.session_state.messages) >= max_messages:
    st.info(
        "æ‚¨çš„ä½¿ç”¨æ¬¡æ•°è¿‡å¤šäº†ï¼Œè¯·ä¼‘æ¯ä¸€ä¸‹ï¼Œä¼‘æ¯å®Œåï¼Œè¯·é‡æ–°æ‰“å¼€ç½‘é¡µï¼Œç»§ç»­ä½¿ç”¨"
    )
else:
    # è·å–ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨å¯¹åˆ˜å¿—æ˜è€å¸ˆæ–‡ç« çš„ç–‘é—®ï¼Œå¸Œæœ›èƒ½å¸®æ‚¨è§£æƒ‘"):
        # è·å–ç›¸å…³æ–‡æ¡£
        docs = get_documents(question, st.session_state.db)

        # å°†ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
        st.session_state.messages.append({"role": "user", "content": question})

        # åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        with st.chat_message("user"):
            st.markdown(question)



with st.chat_message("assistant"):
    full_response = ""
    response = get_response(question, docs)
    full_response += response
    sources = "\n\n".join([f"ğŸ“š æ¥æº {i + 1}: { remain_last( d.metadata['source'] ) } ç¬¬ {d.metadata['page']}é¡µ" for i, d in enumerate(docs)])
    message_placeholder = st.empty()
    message_placeholder.markdown(sources)
st.session_state.messages.append(
    {"role": "assistant", "content": full_response}
)
